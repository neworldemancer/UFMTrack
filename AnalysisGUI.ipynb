{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:22.051186Z",
     "iopub.status.busy": "2024-05-28T12:55:22.050187Z",
     "iopub.status.idle": "2024-05-28T12:55:22.086163Z",
     "shell.execute_reply": "2024-05-28T12:55:22.085166Z",
     "shell.execute_reply.started": "2024-05-28T12:55:22.051186Z"
    }
   },
   "source": [
    "import os, sys, traceback\n",
    "from datetime import datetime\n",
    "\n",
    "lib_root = r'g:\\IVFCA\\UFMTrack'\n",
    "sys.path.append(lib_root) # see the GitHub referenced in the paper"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:23.265388Z",
     "iopub.status.busy": "2024-05-28T12:55:23.264390Z",
     "iopub.status.idle": "2024-05-28T12:55:23.344336Z",
     "shell.execute_reply": "2024-05-28T12:55:23.343337Z",
     "shell.execute_reply.started": "2024-05-28T12:55:23.265388Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:24.254738Z",
     "iopub.status.busy": "2024-05-28T12:55:24.253738Z",
     "iopub.status.idle": "2024-05-28T12:55:46.050405Z",
     "shell.execute_reply": "2024-05-28T12:55:46.049403Z",
     "shell.execute_reply.started": "2024-05-28T12:55:24.254738Z"
    }
   },
   "source": [
    "from GUFMTrack import *\n",
    "from UFMAna import *\n",
    "from config_manager import ConfigManager as cfg\n",
    "\n",
    "from time import time as timer\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout, HBox, VBox, Text, IntText, Button, Output, HTML, SelectMultiple\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import traceback\n",
    "import datetime\n",
    "\n",
    "import pathlib as pl\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:26:48.680596700Z",
     "start_time": "2025-02-20T11:19:23.301192Z"
    }
   },
   "source": [
    "# Processing constants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:46.178320Z",
     "iopub.status.busy": "2024-05-28T12:55:46.177320Z",
     "iopub.status.idle": "2024-05-28T12:55:46.333218Z",
     "shell.execute_reply": "2024-05-28T12:55:46.332219Z",
     "shell.execute_reply.started": "2024-05-28T12:55:46.178320Z"
    }
   },
   "source": [
    "def prep_cfg(accumulation_end=30, start_proc_to_accumulation_end_tf=7):\n",
    "    __ACCUMULATION_END = accumulation_end\n",
    "    #start_proc_to_accumulation_end_tf = 7\n",
    "\n",
    "    __DT_OFFSET = __ACCUMULATION_END - start_proc_to_accumulation_end_tf\n",
    "    __ACCUMULATION_END = __ACCUMULATION_END - __DT_OFFSET\n",
    "\n",
    "    print(f'__ACCUMULATION_END={__ACCUMULATION_END}')\n",
    "\n",
    "    cfgm = cfg()\n",
    "    cfgm.T_ACCUMULATION_END = __ACCUMULATION_END\n",
    "\n",
    "    cfgm.CLASSIFIER_DET = lib_root + '\\\\' + cfgm.CLASSIFIER_DET\n",
    "    cfgm.CLASSIFIER_NAC = lib_root + '\\\\' + cfgm.CLASSIFIER_NAC\n",
    "\n",
    "    print(f'T_END_TO_COMPLETE = {cfgm.T_END_TO_COMPLETE}, T_ACCUMULATION_COMPLETE = {cfgm.T_ACCUMULATION_COMPLETE}')\n",
    "\n",
    "    return cfgm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cfgm = prep_cfg(accumulation_end=30, start_proc_to_accumulation_end_tf=7)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataAnalysisConfig:\n",
    "    seg_ds_path: str = os.path.abspath('../../datasets_seg')\n",
    "    ds_inf_path: str = os.path.abspath('../../datasets_seg/info.txt')\n",
    "    ana_ds_path: str = os.path.abspath('../../datasets_ana')\n",
    "    ana_db_path: str = os.path.abspath('../../datasets_ana/ana_db.csv')\n",
    "\n",
    "\n",
    "# methods for loading and storing the config to file\n",
    "_cfg_filename = 'ana_cfg.json'\n",
    "\n",
    "\n",
    "def load_analysis_cfg(cfg_path=None):\n",
    "    cfg_path = cfg_path or os.path.join(os.path.abspath(os.path.curdir), _cfg_filename)\n",
    "    if os.path.exists(cfg_path):\n",
    "        with open(cfg_path, 'rt') as f:\n",
    "            cfg = DataAnalysisConfig(**json.load(f))\n",
    "    else:\n",
    "        cfg = DataAnalysisConfig()\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def save_analysis_cfg(cfg, cfg_path=None):\n",
    "    cfg_path = cfg_path or os.path.join(os.path.abspath(os.path.curdir), _cfg_filename)\n",
    "    with open(cfg_path, 'wt') as f:\n",
    "        # human-readable, 4 spaces indentation\n",
    "        json.dump(asdict(cfg), f, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_info_file(ds_inf_file_path):\n",
    "    try:\n",
    "        with open(ds_inf_file_path, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        lines = []\n",
    "    ds_inf = {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        subs = line.split('-')\n",
    "        k = int(subs[0].strip())\n",
    "        v = '-'.join(subs[1:])\n",
    "\n",
    "        ds_inf[k] = v.strip()\n",
    "    return ds_inf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_ana_db(da_cfg: DataAnalysisConfig):\n",
    "    ana_db_path = pl.Path(da_cfg.ana_db_path)\n",
    "    if ana_db_path.exists():\n",
    "        ana_db_df = pd.read_csv(ana_db_path)\n",
    "    else:\n",
    "       ana_db_df = None\n",
    "    return ana_db_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sync_analysis_datasets(da_cfg: DataAnalysisConfig):\n",
    "    # 1. load datasets info\n",
    "    ds_inf = read_info_file(da_cfg.ds_inf_path)\n",
    "\n",
    "    # 2. read csv db:\n",
    "    ana_db_df = get_ana_db(da_cfg)\n",
    "    ana_db_df = ana_db_df if ana_db_df is not None else pd.DataFrame(columns=['ds_id', 'ds_name', 'experimentor', 'date', 'details'])\n",
    "\n",
    "    seg_dir = pl.Path(da_cfg.seg_ds_path)\n",
    "    ana_dir = pl.Path(da_cfg.ana_ds_path)\n",
    "\n",
    "    print(f'ds_inf:')\n",
    "    db_updated = False\n",
    "    for ds_id, ds_name in ds_inf.items():\n",
    "        print(f'\\t {ds_id}: \"{ds_name}\"', end='')\n",
    "\n",
    "        # 2. create datasets in the analysis folder if files don't exist and prefill the csv accordingly\n",
    "        ds_dir_name = f'{ds_id:03d}'\n",
    "        seg_ds_dir = seg_dir / ds_dir_name\n",
    "        ana_ds_dir = ana_dir / ds_dir_name\n",
    "\n",
    "        cells_file = seg_ds_dir / 'segmentation' / 'cells' / 'tr_cells_tmp.dat'\n",
    "        tgt_cells_file = ana_ds_dir / 'tr_cells_tmp.dat'\n",
    "\n",
    "        if not cells_file.exists():\n",
    "            # print(f' <- ! skipping - no cells file ({str(cells_file)})')\n",
    "            print()\n",
    "            continue\n",
    "\n",
    "        if not tgt_cells_file.exists():\n",
    "            tgt_cells_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # 3. copy the cells file to the analysis folder\n",
    "            shutil.copy(cells_file, tgt_cells_file)\n",
    "            print(f' <- new file copied ', end='')\n",
    "\n",
    "        # 4. update the db\n",
    "        if ds_id not in ana_db_df['ds_id'].values:\n",
    "            ana_db_df = ana_db_df.append({'ds_id': ds_id, 'ds_name': ds_name}, ignore_index=True)\n",
    "\n",
    "            print(f' <- db record added', end='')\n",
    "            db_updated = True\n",
    "\n",
    "        print()\n",
    "\n",
    "    # 5. save the db\n",
    "    ana_db_path = pl.Path(da_cfg.ana_db_path)\n",
    "    ana_db_df.to_csv(ana_db_path, index=False)\n",
    "\n",
    "    # 6. return if the db was updated\n",
    "    if db_updated:\n",
    "        print(f'\\nUpdated db: {ana_db_path}'\n",
    "              f'\\nPlease fill the missing information in the db file manually before continuing')\n",
    "    return db_updated\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_da_cfg: DataAnalysisConfig or None = None\n",
    "\n",
    "\n",
    "def save_with_widgets():\n",
    "    \"\"\"\n",
    "    Saving widgets state for history hack, reused from\n",
    "    https://stackoverflow.com/questions/59123005/how-to-save-state-of-ipython-widgets-in-jupyter-notebook-using-python-code\n",
    "    \"\"\"\n",
    "    code = '<script>Jupyter.menubar.actions._actions[\"widgets:save-with-widgets\"].handler()</script>'\n",
    "    display(HTML(code))\n",
    "\n",
    "\n",
    "def get_style():\n",
    "    return HTML('''\n",
    "    <style>\n",
    "        .widget-label { min-width: 200px !important; }\n",
    "    </style>''')\n",
    "\n",
    "\n",
    "def analysis_gui():\n",
    "    # 0. Load config\n",
    "    da_cfg = load_analysis_cfg()\n",
    "\n",
    "    # 1. Directory path input string \"Segmentation datasets path\"\n",
    "    seg_ds_path = Text(value=da_cfg.seg_ds_path, description='Segmentation datasets path:', disabled=False,\n",
    "                       layout=Layout(width='600px'))\n",
    "\n",
    "    # 2. File path with datasets IDs list - text human readable file with dataset id - names pairs like \" 85 - Untreated_2024.06.18\"\n",
    "    ds_inf_path = Text(value=da_cfg.ds_inf_path, description='Datasets info path:', disabled=False,\n",
    "                       layout=Layout(width='600px'))\n",
    "\n",
    "    # 3. Directory path input string \"Analysis datasets path\"\n",
    "    ana_ds_path = Text(value=da_cfg.ana_ds_path, description='Analysis datasets path:', disabled=False,\n",
    "                       layout=Layout(width='600px'))\n",
    "\n",
    "    # 4. File path \"Analysis DB path\"\n",
    "    ana_db_path = Text(value=da_cfg.ana_db_path, description='Analysis DB path:', disabled=False,\n",
    "                       layout=Layout(width='600px'))\n",
    "\n",
    "    # 5. Button \"Process\"\n",
    "    process_btn = Button(description='Configure and sync', layout=Layout(width='600px'))\n",
    "\n",
    "    display_styles = get_style()\n",
    "\n",
    "    # 6. Output - text box with the progress\n",
    "    out = Output()\n",
    "\n",
    "    def on_process_click(b):\n",
    "        with out:\n",
    "            try:\n",
    "                print('Configuring...')\n",
    "                # save config\n",
    "                da_cfg.seg_ds_path = seg_ds_path.value\n",
    "                da_cfg.ds_inf_path = ds_inf_path.value\n",
    "                da_cfg.ana_ds_path = ana_ds_path.value\n",
    "                da_cfg.ana_db_path = ana_db_path.value\n",
    "\n",
    "                save_analysis_cfg(da_cfg)\n",
    "                # print('Saved cfg')\n",
    "\n",
    "                ds_inf = read_info_file(da_cfg.ds_inf_path)\n",
    "\n",
    "                sync_analysis_datasets(da_cfg)\n",
    "\n",
    "                save_with_widgets()\n",
    "\n",
    "                global _da_cfg\n",
    "                _da_cfg = da_cfg\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error:', e)\n",
    "                trace_str = traceback.format_exc()\n",
    "                print(trace_str, flush=True)\n",
    "\n",
    "    process_btn.on_click(on_process_click)\n",
    "\n",
    "    controls_b = VBox([\n",
    "        display_styles,\n",
    "        seg_ds_path,\n",
    "        ds_inf_path,\n",
    "        ana_ds_path,\n",
    "        ana_db_path,\n",
    "        process_btn,\n",
    "        out\n",
    "    ])\n",
    "\n",
    "    display(controls_b)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_gui()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analysis GUI:\n",
    "#\n",
    "# Load the datasets info from the db file\n",
    "# add study (name, user, date, details, n conditions)\n",
    "# datasets panel - populate from the db, filter text field for the dataset name and other fields\n",
    "# add conditions to the study - adds panels for each condition with title and add/remove buttons - adds selected from all\n",
    "# add datasets to the conditions (from the db)\n",
    "# save the study_info db in the studies folder as csv\n",
    "# update the study record in the studies db csv\n",
    "# reprocess check box\n",
    "\n",
    "# 1. make a function creating pannel with all dataset given the ana_db_df\n",
    "def get_ds_panel(ana_db_df: pd.DataFrame):\n",
    "    # needs to have a filter textbox and a list of datasets that can be selected with ctrl+click\n",
    "    formated_ids_dict = {ds_id: f'[{ds_id}] {ds_name}: {ds_exp}, \"{ds_date}\" ({ds_det})' for ds_id, ds_name, ds_exp, ds_date, ds_det in zip(\n",
    "        ana_db_df['ds_id'].values,\n",
    "        ana_db_df['ds_name'].values,\n",
    "        ana_db_df['experimentor'].values,\n",
    "        ana_db_df['date'].values,\n",
    "        ana_db_df['details'].values)\n",
    "                         }\n",
    "    filter_text = Text(value='', description='Filter:', disabled=False, layout=Layout(width='600px'))\n",
    "    datasets_list = SelectMultiple(options=formated_ids_dict.values(), layout=Layout(width='600px', height='430px'))\n",
    "    style = get_style()\n",
    "\n",
    "    # handle the filter - on change of the filter text, filter the datasets_list\n",
    "    def filter_datasets_list(change):\n",
    "        filter_text_val = filter_text.value\n",
    "        if not filter_text_val:\n",
    "            datasets_list.options = formated_ids_dict.values()\n",
    "        else:\n",
    "            datasets_list.options = [v for v in formated_ids_dict.values() if filter_text_val.lower() in v.lower()]\n",
    "\n",
    "    filter_text.observe(filter_datasets_list, names='value')\n",
    "\n",
    "\n",
    "    ds_panel = VBox([style, filter_text, datasets_list])\n",
    "    return ds_panel\n",
    "\n",
    "def get_ds_panel_selection(panel):\n",
    "    selected_options = panel.children[2].value\n",
    "\n",
    "    selected_ids = [opt.split(']')[0][1:] for opt in selected_options]\n",
    "\n",
    "    #make dict of pairs form the selected_ids->selected_options\n",
    "    seleced_dict = {id:opt for id, opt in zip(selected_ids, selected_options)}\n",
    "\n",
    "    return seleced_dict\n",
    "\n",
    "def get_cond_panel_list(panel):\n",
    "    panel_options = panel.children[2].options\n",
    "\n",
    "    selected_ids = [opt.split(']')[0][1:] for opt in panel_options]\n",
    "\n",
    "    #make dict of pairs form the selected_ids->selected_options\n",
    "    seleced_dict = {id:opt for id, opt in zip(selected_ids, panel_options)}\n",
    "\n",
    "    return seleced_dict\n",
    "\n",
    "def get_condition_panel(condition_id, ds_panel, study_datasets, status_bar):\n",
    "    # given the condition_id and the ds_panel, create a panel with the condition name text field, SelectMultiple (empty) and two buttons below on one row add and remove\n",
    "    condition_name = Text(value=f'Condition {condition_id}', description='Condition name:', disabled=False, layout=Layout(width='600px'))\n",
    "    cond_ds_list = SelectMultiple(options=[], layout=Layout(width='600px', height='400px'))\n",
    "    add_btn = Button(description='Add', layout=Layout(width='300px'))\n",
    "    rem_btn = Button(description='Remove', layout=Layout(width='300px'))\n",
    "    style = get_style()\n",
    "\n",
    "    button_panel = HBox([add_btn, rem_btn])\n",
    "\n",
    "    cond_panel = VBox([style, condition_name, cond_ds_list, button_panel])\n",
    "\n",
    "    # on add button click, add the selected datasets from the ds_panel to the cond_ds_list\n",
    "\n",
    "    def find_overlaps():\n",
    "        # 1. fill dict condition name->list of ds_ids for all condition panels in study_datasets\n",
    "        cond_ds_dict = {}\n",
    "        for cond_panel_i in study_datasets.children[1:]:\n",
    "            cond_name = cond_panel_i.children[1].value\n",
    "            cond_ds_dict[cond_name] = list(get_cond_panel_list(cond_panel_i).keys())\n",
    "\n",
    "        # print to status bar the cond_ds_dict\n",
    "        status_bar.value = f'{cond_ds_dict}'\n",
    "\n",
    "        # 2. check for overlaps in the datasets between the conditions\n",
    "        overlaps = {}\n",
    "        for i, (cond_name, ds_ids) in enumerate(cond_ds_dict.items()):\n",
    "            for j, (cond_name_j, ds_ids_j) in enumerate(cond_ds_dict.items()):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                overlap = set(ds_ids) & set(ds_ids_j)\n",
    "                if overlap:\n",
    "                    overlaps[(cond_name, cond_name_j)] = overlap\n",
    "\n",
    "        # print to status bar the overlaps\n",
    "        if overlaps:\n",
    "            # set value in red color\n",
    "            overlap_info = ''\n",
    "            for (c1, c2), ds_ids in overlaps.items():\n",
    "                overlap_info += f'{c1} - {c2}: datasets ids {\" ,\".join(ds_ids)}; '\n",
    "            status_bar.value = f'<font color=\"red\">Overlaps between conditions detected:</font>{overlap_info}'\n",
    "\n",
    "        else:\n",
    "            status_bar.value = 'OK'\n",
    "\n",
    "    def on_add_btn_click(b):\n",
    "        selected_ds = get_ds_panel_selection(ds_panel)\n",
    "        options = list(cond_ds_list.options) + list(selected_ds.values())\n",
    "        options = list(set(options))\n",
    "        cond_ds_list.options = options\n",
    "\n",
    "        find_overlaps()\n",
    "\n",
    "\n",
    "    add_btn.on_click(on_add_btn_click)\n",
    "\n",
    "\n",
    "    # on remove button click, remove the selected datasets from the cond_ds_list\n",
    "    def on_rem_btn_click(b):\n",
    "        selected_options = cond_ds_list.value\n",
    "        cond_ds_list.options = [opt for opt in cond_ds_list.options if opt not in selected_options]\n",
    "        find_overlaps()\n",
    "\n",
    "    rem_btn.on_click(on_rem_btn_click)\n",
    "\n",
    "    return cond_panel\n",
    "\n",
    "def get_study_ui(ana_db_df):\n",
    "    # create a panel with the study name, user, date, details (text fields) and number of conditions (int selector) all in vbox `study_info`\n",
    "    # below it a h_box `study_datasets` with ds_panel\n",
    "\n",
    "    study_name = Text(value='', description='Study name:', disabled=False, layout=Layout(width='600px'))\n",
    "    user = Text(value='', description='User:', disabled=False, layout=Layout(width='600px'))\n",
    "    date = Text(value=datetime.datetime.now().strftime('%Y.%m.%d'),\n",
    "                description='Date:', disabled=False, layout=Layout(width='600px'))\n",
    "    details = Text(value='', description='Details:', disabled=False, layout=Layout(width='600px'))\n",
    "    n_conditions = IntText(value=1, description='Number of conditions:', disabled=False, layout=Layout(width='600px'))\n",
    "\n",
    "    style = get_style()\n",
    "\n",
    "    study_info = VBox([style, study_name, user, date, details, n_conditions])\n",
    "\n",
    "    # spacer with horizontal line\n",
    "    spacer_vert = HTML(value='<hr>', description='', layout=Layout(width='100vp', height='20px'))\n",
    "\n",
    "    ds_panel = get_ds_panel(ana_db_df)\n",
    "\n",
    "    study_datasets = HBox([ds_panel])\n",
    "\n",
    "    # add vbox with status bar label for warnigns etc and the Process button\n",
    "    status_bar = HTML(value='OK', description='Status:', layout=Layout(width='100vp'))\n",
    "    proc_btn = Button(description='Process', layout=Layout(width='100px'))\n",
    "    footer = VBox([style, status_bar, proc_btn])\n",
    "\n",
    "    study_ui = VBox([study_info, spacer_vert, study_datasets, spacer_vert, footer])\n",
    "\n",
    "\n",
    "    # upon change of the n_conditions, add or remove the condition panels to the `study_datasets`\n",
    "\n",
    "    def on_n_conditions_change(change):\n",
    "        n_conds = n_conditions.value\n",
    "        n_children = len(study_datasets.children)\n",
    "        if n_conds > n_children-1:  # -1 for the ds_panel\n",
    "            for i in range(n_children-1, n_conds):\n",
    "                cond_panel = get_condition_panel(i, ds_panel, study_datasets, status_bar)\n",
    "                study_datasets.children = study_datasets.children + (cond_panel,)\n",
    "        elif n_conds <= n_children-1:\n",
    "            study_datasets.children = study_datasets.children[:n_conds+1]\n",
    "\n",
    "    n_conditions.observe(on_n_conditions_change, names='value')\n",
    "\n",
    "    # init the condition panels - according to initial n_conditions value\n",
    "    on_n_conditions_change(None)\n",
    "\n",
    "    # handle the process button click\n",
    "    def on_proc_btn_click(b):\n",
    "        study_info_dict = get_study_info(study_ui)\n",
    "        print(study_info_dict)\n",
    "        # set same to status bar\n",
    "        status_bar.value = f'Study info: {study_info_dict}'\n",
    "\n",
    "    proc_btn.on_click(on_proc_btn_click)\n",
    "\n",
    "    return study_ui\n",
    "\n",
    "def get_study_info(study_ui):\n",
    "    study_info = study_ui.children[0]\n",
    "    study_datasets = study_ui.children[2]\n",
    "\n",
    "    study_name = study_info.children[1].value\n",
    "    user = study_info.children[2].value\n",
    "    date = study_info.children[3].value\n",
    "    details = study_info.children[4].value\n",
    "    n_conditions = study_info.children[5].value\n",
    "\n",
    "    conditions = []\n",
    "    for cond_panel in study_datasets.children[1:]:\n",
    "        cond_name = cond_panel.children[1].value\n",
    "        cond_ds = list(get_cond_panel_list(cond_panel).keys())\n",
    "        conditions.append((cond_name, cond_ds))\n",
    "\n",
    "    study_info_dict = {\n",
    "        'study_name': study_name,\n",
    "        'user': user,\n",
    "        'date': date,\n",
    "        'details': details,\n",
    "        'n_conditions': n_conditions,\n",
    "        'conditions': conditions\n",
    "    }\n",
    "\n",
    "    return study_info_dict\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ana_db_df = get_ana_db(_da_cfg)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "study_ui = get_study_ui(ana_db_df)\n",
    "display(study_ui)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Datasets info"
  },
  {
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:46.893848Z",
     "iopub.status.busy": "2024-05-28T12:55:46.893848Z",
     "iopub.status.idle": "2024-05-28T12:55:47.018767Z",
     "shell.execute_reply": "2024-05-28T12:55:47.017767Z",
     "shell.execute_reply.started": "2024-05-28T12:55:46.893848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets_ids = [472, 473, 474, 475, 476, 477, 478, 479, ]\n",
    "\n",
    "datasets_acc_complete_t = [start_proc_to_accumulation_end_tf]*len(datasets_ids)\n",
    "\n",
    "excl_ds = []\n",
    "\n",
    "datasets_acc_complete_t = [t for ds_idx, t in zip(datasets_ids, datasets_acc_complete_t) if ds_idx not in excl_ds]\n",
    "\n",
    "datasets_ids = [ds_idx for ds_idx in datasets_ids if ds_idx not in excl_ds]\n",
    "\n",
    "\n",
    "assert len(datasets_ids) == len(datasets_acc_complete_t)\n",
    "\n",
    "# human readable description of ds type\n",
    "# ToDo: selection interface\n",
    "condition_id_to_condition_name = {0: 'BMEC'}\n",
    "\n",
    "condition_id_to_ds_id = {0:datasets_ids}\n",
    "\n",
    "condition_id_to_ds_id = {k:[vi for vi in v if vi not in excl_ds] for k, v in condition_id_to_ds_id.items()}\n",
    "\n",
    "ds_id_to_condition_id = {ds_id:cond_id for cond_id, ds_ids in condition_id_to_ds_id.items() for ds_id in ds_ids}\n",
    "\n",
    "datasets_dir = r'' # path to the datasets\n",
    "plot_dir=os.path.join(datasets_dir, 'ds_plot')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:47.020765Z",
     "iopub.status.busy": "2024-05-28T12:55:47.020765Z",
     "iopub.status.idle": "2024-05-28T12:55:47.143685Z",
     "shell.execute_reply": "2024-05-28T12:55:47.142685Z",
     "shell.execute_reply.started": "2024-05-28T12:55:47.020765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ds_info = gen_ds_info(txt_filename=r'q:\\deep\\BBB_Home\\jpnb\\BBB_data_proc\\info')\n",
    "# save_pckl(ds_info, os.path.join(datasets_dir, 'ds_info.pckl'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:47.144683Z",
     "iopub.status.busy": "2024-05-28T12:55:47.144683Z",
     "iopub.status.idle": "2024-05-28T12:55:47.288590Z",
     "shell.execute_reply": "2024-05-28T12:55:47.286591Z",
     "shell.execute_reply.started": "2024-05-28T12:55:47.144683Z"
    }
   },
   "source": [
    "ds_info = load_pckl(os.path.join(datasets_dir, 'ds_info.pckl'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving crossings & Track analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:47.299582Z",
     "iopub.status.busy": "2024-05-28T12:55:47.298583Z",
     "iopub.status.idle": "2024-05-28T12:55:47.439491Z",
     "shell.execute_reply": "2024-05-28T12:55:47.439491Z",
     "shell.execute_reply.started": "2024-05-28T12:55:47.298583Z"
    },
    "scrolled": true
   },
   "source": [
    "datasets_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T12:55:47.442489Z",
     "iopub.status.busy": "2024-05-28T12:55:47.441488Z",
     "iopub.status.idle": "2024-05-28T15:18:11.315321Z",
     "shell.execute_reply": "2024-05-28T15:18:11.314335Z",
     "shell.execute_reply.started": "2024-05-28T12:55:47.442489Z"
    },
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "# process with proper accumulation end time set:\n",
    "failed_ds = []\n",
    "n_att = 1\n",
    "for acc_complete_t, ds_id in zip(datasets_acc_complete_t, datasets_ids):\n",
    "    cfgm.T_ACCUMULATION_END = acc_complete_t - cfgm.T_END_TO_COMPLETE\n",
    "    \n",
    "    condition_id_to_ds_id_l = {0: [ds_id]}\n",
    "    condition_id_to_condition_name_l = {0: 'T'}\n",
    "\n",
    "    ds_id_to_condition_id_l = {ds_id:0}\n",
    "    \n",
    "    for att in range(n_att):\n",
    "        try:\n",
    "            tas_single = process_datasets([ds_id], \n",
    "                                     condition_id_to_condition_name_l, condition_id_to_ds_id_l, ds_id_to_condition_id_l,\n",
    "                                     datasets_dir, plot_dir,\n",
    "                                     use_ds_priors=False,\n",
    "                                     skip_processed_xing=False, skip_processed_tr_ana=False,\n",
    "                                     no_transm_mode=False # option for Binding assys: True - no transmigration in this case possible.\n",
    "                                    )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'\\nRaised exception in {ds_id}\\n', e)\n",
    "            print(traceback.format_exception(None, # <- type(e) by docs, but ignored \n",
    "                                             e, e.__traceback__),\n",
    "                  file=sys.stderr, flush=True)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        failed_ds.append(ds_id)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T15:18:11.316321Z",
     "iopub.status.busy": "2024-05-28T15:18:11.316321Z",
     "iopub.status.idle": "2024-05-28T15:18:14.495231Z",
     "shell.execute_reply": "2024-05-28T15:18:14.494232Z",
     "shell.execute_reply.started": "2024-05-28T15:18:11.316321Z"
    }
   },
   "source": [
    "failed_ds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T15:18:14.497229Z",
     "iopub.status.busy": "2024-05-28T15:18:14.496229Z",
     "iopub.status.idle": "2024-05-28T16:22:30.771233Z",
     "shell.execute_reply": "2024-05-28T16:22:30.770231Z",
     "shell.execute_reply.started": "2024-05-28T15:18:14.497229Z"
    }
   },
   "source": [
    "# read processed and fill the tas_all structure\n",
    "filtered_condition_id_to_ds_id = {k: [vi for vi in v if vi not in failed_ds] for k, v in condition_id_to_ds_id.items()}\n",
    "tas_all = process_datasets([ids for ids in datasets_ids if ids not in failed_ds], \n",
    "                             condition_id_to_condition_name, filtered_condition_id_to_ds_id, ds_id_to_condition_id,\n",
    "                             datasets_dir, plot_dir,\n",
    "                             use_ds_priors=False,\n",
    "                             skip_processed_xing=True, skip_processed_tr_ana=True,\n",
    "                             no_transm_mode=False # option for Binding assys: True - no transmigration in this case possible.\n",
    "                            )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse all (prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of dataset into groups"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T16:22:30.774230Z",
     "iopub.status.busy": "2024-05-28T16:22:30.773245Z",
     "iopub.status.idle": "2024-05-28T16:22:34.612831Z",
     "shell.execute_reply": "2024-05-28T16:22:34.609828Z",
     "shell.execute_reply.started": "2024-05-28T16:22:30.774230Z"
    }
   },
   "source": [
    "condition_id_to_ds_id_g3 = {\n",
    "    0:[472, 475, 478, 474, ],\n",
    "    1:[473, 476, 477, 479, ]\n",
    "    \n",
    "}\n",
    "\n",
    "comparisson_groups_g3 = {\n",
    "    0: {'condition_id': 0, 'name': 'AAA'},\n",
    "    1: {'condition_id': 1, 'name': 'BBB'},\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze G3 (merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we create a data structure where for each of the measured condition we jave n+1 dict for each condition + joined, dict for param_id (str) + human-readable param name + container object with fetchers, store/load to pickle, save all csvs, save plots \n",
    "\n",
    "+ aggregator pipeline class: point to track dirs, list ids, condition map -> run each step, xint-tra, analysis, save plots, gui for map/dir gen, cache tas, save csv; common work_dir - contains existing recoded conditions + tags - date etc, to differentiate. gui suggestes one of existing param names, or allows to create new one. \n",
    "\n",
    ", for migration regimes - just id ref+ map of HR values\n",
    "study - produces folder with links to data (ims+tracks+pta) + plots + csv\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T16:22:34.629158Z",
     "iopub.status.busy": "2024-05-28T16:22:34.628160Z",
     "iopub.status.idle": "2024-05-28T16:22:34.736145Z",
     "shell.execute_reply": "2024-05-28T16:22:34.736145Z",
     "shell.execute_reply.started": "2024-05-28T16:22:34.629158Z"
    }
   },
   "source": [
    "study_dir_g3 = os.path.join(datasets_dir, 'groups_g3_analytics')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T16:22:34.738157Z",
     "iopub.status.busy": "2024-05-28T16:22:34.738157Z",
     "iopub.status.idle": "2024-05-28T16:25:44.689962Z",
     "shell.execute_reply": "2024-05-28T16:25:44.688946Z",
     "shell.execute_reply.started": "2024-05-28T16:22:34.738157Z"
    }
   },
   "source": [
    "analyze_group(tas_all, comparisson_groups_g3, condition_id_to_ds_id_g3, study_dir_g3, show=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T16:25:44.690961Z",
     "iopub.status.busy": "2024-05-28T16:25:44.689962Z",
     "iopub.status.idle": "2024-05-28T16:25:44.796885Z",
     "shell.execute_reply": "2024-05-28T16:25:44.795884Z",
     "shell.execute_reply.started": "2024-05-28T16:25:44.690961Z"
    }
   },
   "source": [
    "nb_end_t = timer()\n",
    "print(f'notebook run time: {(nb_end_t - nb_start_t):.2f} sec')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
